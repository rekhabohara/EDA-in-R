summary(lmModel_second)
# Validating Regression Coefficients and Models
summary(lmModel_second)
# predicting
model_test$PreditedPrice_2 <- predict(lmModel_second, model_test)
head(model_test[ , c("SalePrice", "PreditedPrice_1",  "PreditedPrice_2")])
# RMSE
actual <- model_test$SalePrice
preds <- model_test$PreditedPrice_2
Mode2_one_RMSE<- RMSE(preds, actual)
#Mode2_one_RMSE_formula <- sqrt(mean((preds - actual)^2))
plot(lmModel_second)
hist(lmModel_First$residuals, color = "grey")
hist(lmModel_second$residuals, color = "grey")
ggplot(lmModel_First, aes(lmModel_First$residuals)) +
geom_histogram(aes(y = ..density..), fill = "#C99800") +
geom_density(color = "blue")
ggplot(lmModel_second, aes(lmModel_second$residuals)) +
geom_histogram(aes(y = ..density..), fill = "#00BCD8") +
geom_density(color = "red")
```
***Step 8:Conclusions***
The RMSE for our training and test sets should be very similar if we  have built a good model. If the RMSE for the test set is much higher than that of the training set, it is likely that we've badly over fit the data, i.e. created a model that tests well in sample, but has little predictive value when tested out of sample.
As per data received from modelling.
lmModel_First:
Residual standard error: 0.1677 on 1458 degrees of freedom
Multiple R-squared:  0.06621,	Adjusted R-squared:  0.06557
F-statistic: 103.4 on 1 and 1458 DF,  p-value: < 2.2e-16
Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are; RMSE is a measure of how to spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit.
Model2 :
Residual standard error: 0.07751 on 1419 degrees of freedom
Multiple R-squared:  0.8059,	Adjusted R-squared:  0.8004
F-statistic: 147.3 on 40 and 1419 DF,  p-value: < 2.2e-16
Lets check Residual graph,model distribution graph with RMSe and R square value with p-value.  Low RMSE and high R square value compared between two above model, Model_second is prefers to predict prices as per changes in variables .
*Code of Ethics*
- I have use this data set only for my assignment purposes
- I get this data set from my university site for my educational purpose only
*References:*
Mostly took reference from EDA course lecture slides,Lab exercises and  university data set and report
*https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data*
*https://www.amazon.com.au/dp/1491910399?tag=guru99-22&geniuslink=true&asin=B01NAJAEN5&revisionId=d56915a7&format=1&depth=1*
*https://www.kaggle.com/c/house-prices-advanced-regressiontechniques/notebooks?sortBy=hotness&group=everyone&pageSize=20&competitionId=5407&language=R*
*Thank you Sir Suang for your guidance in this Semester*
Lets check Residual graph,model distribution graph with RMSe and R square value with p-value.  Low RMSE and high R square value compared between two above model, Model_second is prefers to predict prices as per changes in variables .
*Code of Ethics*
- I have use this data set only for my assignment purposes
- I get this data set from my university site for my educational purpose only
#clean Console  as command (CTRL + L)
cat("\014")
#clean all global variables
rm(list = ls())
#Library:
library(heatmaply)
library (tidyverse)
library (dplyr)
library(corrplot)
library(GGally)
library(caret)
library(modelr)
#Fetch Data
parent_directory <-  dirname(rstudioapi::getSourceEditorContext()$path)
print(parent_directory)
setwd(parent_directory)
csv_files <- list.files(parent_directory, pattern = ".csv" ,full.names = FALSE )
csv_dataframe <- sub(".csv","",csv_files)
print(csv_dataframe)
for (i in 1:length(csv_files)){ # create aloop as per length count of csv file list
df_name <- sub(".csv","", csv_files[i])  #replaces the  match of all elements from loop
file_data<- (csv_files[i]) # Single file name to works
data <- assign(df_name,read_csv(file_data,show_col_types = FALSE))
}
ggplot(data = train , aes(train$SalePrice)) + geom_histogram( bins=30,col="white") + theme_classic() +
xlab("Price of Houses") +
ggtitle("Distribution of house prices irrespective of neighborhoods")
# check for missing values in all columns of the data set
na_values <- data.frame(sapply(train,function(x)sum(is.na(x) )))
colnames(na_values) <- ("Sum")
na_df <- na_values %>% filter(Sum>0)
na_df %>%  mutate(na_percent = (na_df$Sum/nrow(train) )*100)
#Fill missing values
train$LotFrontage[is.na(train$LotFrontage)]<-mean(train$LotFrontage,na.rm=TRUE)
#Check & Replacing NAs missing values  For test*
# check for missing values in all columns of the data set
na_values <- data.frame(sapply(test,function(x)sum(is.na(x) )))
colnames(na_values) <- ("Sum")
na_df <- na_values %>% filter(Sum>0)
na_df %>%  mutate(na_percent = (na_df$Sum/nrow(test) )*100)
#Fill missing values
test$LotFrontage[is.na(test$LotFrontage)]<-mean(test$LotFrontage,na.rm=TRUE)
# check for missing values in all columns of the data set
sapply(train, function(x) sum(is.na(x)))
#Fill missing values for Numeric with the mean of numeric data
num_cols <- names(train)[sapply(train, is.numeric)]
train$LotFrontage[is.na(train$LotFrontage)]<-mean(train$LotFrontage,na.rm=TRUE)
train$GarageYrBlt[is.na(train$GarageYrBlt)]<-mean(train$GarageYrBlt,na.rm=TRUE)
#Fill missing values for categorical with with the highest occurrence
calc_mode <- function(x){
# List the distinct / unique values
distinct_values <- unique(x)
# Count the occurrence of each distinct value
distinct_tabulate <- tabulate(match(x, distinct_values))
# Return the value with the highest occurrence
distinct_values[which.max(distinct_tabulate)]
}
train$BsmtCond = if_else(is.na(train$BsmtCond),
calc_mode(train$BsmtCond),
train$BsmtCond)
train$BsmtQual = if_else(is.na(train$BsmtQual),
calc_mode(train$BsmtQual),
train$BsmtQual)
train$BsmtFinType1 = if_else(is.na(train$BsmtFinType1),
calc_mode(train$BsmtFinType1),
train$BsmtFinType1)
train$BsmtFinType2 = if_else(is.na(train$BsmtFinType2),
calc_mode(train$BsmtFinType2),
train$BsmtFinType2)
train$BsmtExposure = if_else(is.na(train$BsmtExposure),
calc_mode(train$BsmtExposure),
train$BsmtExposure)
train$GarageType = if_else(is.na(train$GarageType),
calc_mode(train$GarageType),
train$GarageType)
train$GarageFinish = if_else(is.na(train$GarageFinish),
calc_mode(train$GarageFinish),
train$GarageFinish)
train$GarageQual = if_else(is.na(train$GarageQual),
calc_mode(train$GarageQual),
train$GarageQual)
train$GarageCond = if_else(is.na(train$GarageCond),
calc_mode(train$GarageCond),
train$GarageCond)
train$Electrical = if_else(is.na(train$Electrical),
calc_mode(train$Electrical),
train$Electrical)
#Drop unnecessary columns with maximum null values and which less  meaningful
drop <- c("Alley","PoolQC","Fence","MiscFeature","Id","FireplaceQu")
train = train[,!(names(train) %in% drop)]
train_data_na <- data.frame(na_count= sapply(train,function(x)sum(is.na(x) )))
sapply(train, function(x) sum(is.na(x)))
# check for missing values in all columns of the data set
sapply(test, function(x) sum(is.na(x)))
calc_mode <- function(x){
# List the distinct / unique values
distinct_values <- unique(x)
# Count the occurrence of each distinct value
distinct_tabulate <- tabulate(match(x, distinct_values))
# Return the value with the highest occurrence
distinct_values[which.max(distinct_tabulate)]
}
#Remove variables that are missing from >90% of all observations.
names(test[colSums(is.na(test))/nrow(data) > 0.9])
# Return the value with the highest occurrence for below categorical data
test$MSZoning = if_else(is.na(test$MSZoning),
calc_mode(test$MSZoning),
test$MSZoning)
test$Utilities = if_else(is.na(test$Utilities),
calc_mode(test$Utilities),
test$Utilities)
test$Exterior1st = if_else(is.na(test$Exterior1st),
calc_mode(test$Exterior1st),
test$Exterior1st)
test$Exterior2nd = if_else(is.na(test$Exterior2nd),
calc_mode(test$Exterior2nd),
test$Exterior2nd)
test$KitchenQual = if_else(is.na(test$KitchenQual),
calc_mode(test$KitchenQual),
test$KitchenQual)
test$Functional = if_else(is.na(test$Functional),
calc_mode(test$Functional),
test$Functional)
test$SaleType = if_else(is.na(test$SaleType),
calc_mode(test$SaleType),
test$SaleType)
test$MasVnrType = if_else(is.na(test$MasVnrType),
calc_mode(test$MasVnrType),
test$MasVnrType)
test$BsmtQual = if_else(is.na(test$BsmtQual),
calc_mode(test$BsmtQual),
test$BsmtQual)
test$BsmtCond = if_else(is.na(test$BsmtCond),
calc_mode(test$BsmtCond),
test$BsmtCond)
test$BsmtExposure = if_else(is.na(test$BsmtExposure),
calc_mode(test$BsmtExposure),
test$BsmtExposure)
test$GarageType = if_else(is.na(test$GarageType),
calc_mode(test$GarageType),
test$GarageType)
test$GarageFinish = if_else(is.na(test$GarageFinish),
calc_mode(test$GarageFinish),
test$GarageFinish)
test$BsmtFinType2 = if_else(is.na(test$BsmtFinType2),
calc_mode(test$BsmtFinType2),
test$BsmtFinType2)
test$GarageQual = if_else(is.na(test$GarageQual),
calc_mode(test$GarageQual),
test$GarageQual)
num_cols <- names(test)[sapply(test, is.numeric)]
print(num_cols)
# for numeric variable lets rplace NAs with  mean value
test$LotFrontage[is.na(test$LotFrontage)]<-mean(test$LotFrontage,na.rm=TRUE)
test$MasVnrArea[is.na(test$MasVnrArea)]<-mean(test$MasVnrArea,na.rm=TRUE)
test$BsmtFinSF1[is.na(test$BsmtFinSF1)]<-mean(test$BsmtFinSF1,na.rm=TRUE)
test$GarageYrBlt  [is.na(test$GarageYrBlt  )]<-mean(test$GarageYrBlt,na.rm=TRUE)
test$BsmtFinSF2 <- mean(is.na(test$BsmtFinSF2))
test$BsmtUnfSF  <- mean(is.na(test$BsmtUnfSF))
test$TotalBsmtSF <- mean(is.na(test$TotalBsmtSF))
test$BsmtFullBath <- mean(is.na(test$BsmtFullBath))
test$BsmtHalfBath <- mean(is.na(test$BsmtHalfBath))
test$GarageCars <- mean(is.na(test$GarageCars))
test$GarageArea <- mean(is.na(test$GarageArea))
test$BedroomAbvGr <- mean(is.na(test$BedroomAbvGr))
test$GarageCond<- mean(is.na(test$GarageCond))
test$BsmtFinType1 <- mean(is.na(test$BsmtFinType1))
test$GarageCond  <- mean(is.na(test$GarageCond))
#Drop unnecessary columns with maximum null values and which seems meaningless
drop <- c("Alley","PoolQC","Fence","MiscFeature","Id","FireplaceQu")
test = test[,!(names(test) %in% drop)]
test_data_na <- data.frame(na_count= sapply(test,function(x)sum(is.na(x) )))
sapply(test, function(x) sum(is.na(x)))
# check for missing values in all columns of the data set
sapply(test, function(x) sum(is.na(x)))
calc_mode <- function(x){
# List the distinct / unique values
distinct_values <- unique(x)
# Count the occurrence of each distinct value
distinct_tabulate <- tabulate(match(x, distinct_values))
# Return the value with the highest occurrence
distinct_values[which.max(distinct_tabulate)]
}
#Remove variables that are missing from >90% of all observations.
names(test[colSums(is.na(test))/nrow(data) > 0.9])
# Return the value with the highest occurrence for below categorical data
test$MSZoning = if_else(is.na(test$MSZoning),
calc_mode(test$MSZoning),
test$MSZoning)
test$Utilities = if_else(is.na(test$Utilities),
calc_mode(test$Utilities),
test$Utilities)
test$Exterior1st = if_else(is.na(test$Exterior1st),
calc_mode(test$Exterior1st),
test$Exterior1st)
test$Exterior2nd = if_else(is.na(test$Exterior2nd),
calc_mode(test$Exterior2nd),
test$Exterior2nd)
test$KitchenQual = if_else(is.na(test$KitchenQual),
calc_mode(test$KitchenQual),
test$KitchenQual)
test$Functional = if_else(is.na(test$Functional),
calc_mode(test$Functional),
test$Functional)
test$SaleType = if_else(is.na(test$SaleType),
calc_mode(test$SaleType),
test$SaleType)
test$MasVnrType = if_else(is.na(test$MasVnrType),
calc_mode(test$MasVnrType),
test$MasVnrType)
test$BsmtQual = if_else(is.na(test$BsmtQual),
calc_mode(test$BsmtQual),
test$BsmtQual)
test$BsmtCond = if_else(is.na(test$BsmtCond),
calc_mode(test$BsmtCond),
test$BsmtCond)
test$BsmtExposure = if_else(is.na(test$BsmtExposure),
calc_mode(test$BsmtExposure),
test$BsmtExposure)
test$GarageType = if_else(is.na(test$GarageType),
calc_mode(test$GarageType),
test$GarageType)
test$GarageFinish = if_else(is.na(test$GarageFinish),
calc_mode(test$GarageFinish),
test$GarageFinish)
test$BsmtFinType2 = if_else(is.na(test$BsmtFinType2),
calc_mode(test$BsmtFinType2),
test$BsmtFinType2)
test$GarageQual = if_else(is.na(test$GarageQual),
calc_mode(test$GarageQual),
test$GarageQual)
num_cols <- names(test)[sapply(test, is.numeric)]
print(num_cols)
# for numeric variable lets rplace NAs with  mean value
test$LotFrontage[is.na(test$LotFrontage)]<-mean(test$LotFrontage,na.rm=TRUE)
test$MasVnrArea[is.na(test$MasVnrArea)]<-mean(test$MasVnrArea,na.rm=TRUE)
test$BsmtFinSF1[is.na(test$BsmtFinSF1)]<-mean(test$BsmtFinSF1,na.rm=TRUE)
test$GarageYrBlt  [is.na(test$GarageYrBlt  )]<-mean(test$GarageYrBlt,na.rm=TRUE)
test$BsmtFinSF2 <- mean(is.na(test$BsmtFinSF2))
test$BsmtUnfSF  <- mean(is.na(test$BsmtUnfSF))
test$TotalBsmtSF <- mean(is.na(test$TotalBsmtSF))
test$BsmtFullBath <- mean(is.na(test$BsmtFullBath))
test$BsmtHalfBath <- mean(is.na(test$BsmtHalfBath))
test$GarageCars <- mean(is.na(test$GarageCars))
test$GarageArea <- mean(is.na(test$GarageArea))
test$BedroomAbvGr <- mean(is.na(test$BedroomAbvGr))
test$GarageCond<- mean(is.na(test$GarageCond))
test$BsmtFinType1 <- mean(is.na(test$BsmtFinType1))
test$GarageCond  <- mean(is.na(test$GarageCond))
#Drop unnecessary columns with maximum null values and which seems meaningless
drop <- c("Alley","PoolQC","Fence","MiscFeature","Id","FireplaceQu")
test = test[,!(names(test) %in% drop)]
test_data_na <- data.frame(na_count= sapply(test,function(x)sum(is.na(x) )))
sapply(test, function(x) sum(is.na(x)))
ggplot(train, mapping = aes(SalePrice))+
geom_boxplot( color ="Red")+
scale_x_continuous(labels = scales::comma)+
theme_minimal()
test[sapply(test, is.character)] <- lapply(test[sapply(test,is.character)],as.factor)
corr_simple <- function(data = train,sig = 0.5){
#convert data to numeric in order to run correlations
#convert to factor first to keep the integrity of the data - each value will become a number rather than turn into NA
df_cor <- data %>% mutate_if(is.character, as.factor)
df_cor <- df_cor %>% mutate_if(is.factor, as.numeric)  #run a correlation and drop the insignificant ones
corr <- cor(df_cor)
#prepare to drop duplicates and correlations of 1
corr[lower.tri(corr,diag = TRUE)] <- NA
#drop perfect correlations
corr[corr == 1] <- NA   #turn into a 3-column table
corr <- as.data.frame(as.table(corr))
#remove the NA values from above
corr <- na.omit(corr)   #select significant values
corr <- subset(corr, abs(Freq) > sig)
#sort by highest correlation
corr <- corr[order(-abs(corr$Freq)),]   #print table
print(corr)  #turn corr back into matrix in order to plot with corrplot
mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="Freq")
#plot correlations visually
corrplot(mtx_corr, is.corr = FALSE, tl.col="black", na.label=" ")
}
corr_simple()
train <- train %>% mutate(SalePrice = log10(SalePrice))
# Histogram of Salesprices
ggplot(train, aes(x = SalePrice))+
geom_histogram(color = "yellow", fill = "pink", bins = 50) +
scale_x_continuous() +
labs(title = "Distribution of house salesprices", x = "Price", y = "Frequency") +
theme_minimal() + scale_x_continuous(labels = scales::comma)
mean(train$SalePrice)
ggplot(data=train, aes(x=Neighborhood, y=(SalePrice/100), fill=Neighborhood))+
geom_boxplot()+
scale_y_continuous(name="Sale Price")+
theme(axis.text.x  = element_text(angle=90, vjust=0.5))
Neighborhood = tapply(train$SalePrice, train$Neighborhood, median)
Neighborhood = sort(Neighborhood, decreasing = TRUE)
dotchart(Neighborhood, pch = 21, bg = "purple1",
cex = 0.85,
xlab="Average price of a house",
main = "Which is Most expensive neighborhood sold with the most highest price ?")
Monthly_sales<- train %>%
group_by(MoSold)%>%
summarise( Sales = n())%>%
mutate(Month_sold = factor(month.abb[MoSold], levels = month.abb))
Monthly_sales
ggplot(data = Monthly_sales, mapping = aes(x= Month_sold, y= Sales,fill = Month_sold, label = Sales))+
geom_col(show.legend = FALSE)+
geom_text(aes(label = Sales, color = Month_sold, vjust=-0.2),show.legend = FALSE )+
theme_minimal()+
labs(title = 'Individual  properties sold in Ames')+
xlab(element_blank())+
ylab('Number of Properties')
sold_house<- train %>%
group_by(YrSold)%>%
summarise(Sales = n())
sold_house
ggplot(sold_house)+
geom_col(aes(x = YrSold,y = Sales),color='red',fill='yellow')+
theme_minimal()+
labs(title="Number of house sold each year",x="Year",y="Count")
ggplot(data = train, aes(HouseStyle, SalePrice)) +
geom_jitter() + geom_smooth(method="lm")+geom_line(col="red")
theme(axis.text.x = element_text(angle = 90)) +
theme(legend.position = "none") +
ggtitle("Median Price in relation to HouseStyle in Ames,Iowa") +
xlab("HouseStyle in Ames, Iowa") +
ylab("Median Price") +
theme(plot.title = element_text(size = 5)) +
theme(axis.title = element_text(size = 5))
#BsmtUnfSF
train$BsmtFinSF <- train$TotalBsmtSF-train$BsmtUnfSF
test$BsmtFinSF <- test$TotalBsmtSF-test$BsmtUnfSF
train %>%  mutate(train$BsmtFinSF)
ggplot(train , aes(BsmtFinSF)) + geom_histogram(bins = 60, col = "white") + theme_classic() +
xlab("Price of Houses") +
ggtitle("Distribution of house prices irrespective of BsmtFinSF")
ames_train_BsmtFinSF <- train %>%
group_by(BsmtFinSF) %>%
summarise(Price_BsmtFinSF= median(train$SalePrice)) %>%
arrange(Price_BsmtFinSF)
#490.189
ggplot(train)+
geom_histogram(aes(LotArea),bins=40,color='red',fill='brown')+
geom_vline(aes(xintercept=mean(LotArea)),
color="blue", linetype="dashed", size=1)+
theme_minimal()+
labs(title="Lot Area Distribution",x="Lot Area (sq.ft.)",y="Number of Houses")
mean(train$LotArea)
Foundation_type<- data.frame(table(train$Foundation))
colnames(Foundation_type)<- c('Foundation','count')
Foundation_type
ggplot(Foundation_type,mapping = aes(x=Foundation, y=count,fill=Foundation))+
geom_col()+
theme_minimal()+
labs(title = 'Distribution of Foundation')
train$year_fraction <- train$YrSold + train$MoSold/12
ggplot(train , aes(year_fraction)) + geom_histogram(bins = 60, col= "white" ) + theme_classic() +
xlab("Price of Houses") +
ggtitle("Distribution of house prices irrespective of year_fraction")
ames_train_year_fraction <- train %>%
group_by(HouseStyle) %>%
summarise(Price_year_fraction = median(train$year_fraction)) %>%
arrange(Price_year_fraction)
# YearBuilt
ggplot(train)+
geom_density(aes(YearBuilt),color='green',fill='orange')+
theme_minimal()+
labs(title="Year Built Distribution",x="Year Built",y="Density")
ggplot(train,aes(YearBuilt,SalePrice))+
geom_point()+
geom_smooth(method='lm', se=FALSE)+
scale_y_continuous(labels = scales::comma)+
theme_minimal()+
labs(title="Sale Price VS YearBuilt ",x="YearBuilt",y="Sale Price ($)")
ggplot(data = train, aes(BedroomAbvGr, SalePrice)) +
geom_jitter() + geom_smooth(method="lm")+ geom_point(col="red")
theme(axis.text.x = element_text(angle = 90)) +
theme(legend.position = "none") +
ggtitle("Median Price in relation to BedroomAbvGr in Ames,Iowa") +
xlab("BedroomAbvGr in Ames, Iowa") +
ylab("Median Price") +
theme(plot.title = element_text(size = 5)) +
theme(axis.title = element_text(size = 5))
cor(train$SalePrice,train$BedroomAbvGr)
train <- train %>% mutate(Sale_Price = log10(SalePrice))
ggplot(train,aes(LotArea,Sale_Price))+
geom_point()+
geom_smooth(method='lm', se=FALSE)+
scale_y_continuous(labels = scales::comma)+
theme_minimal()+
labs(title="Sale Price VS Lot Area",x="Lot Area (sq. ft.)",y="Sale Price ($)")
train %>% summarise(cor(LotArea, Sale_Price))
mod <- lm(SalePrice ~ YearBuilt ,data= train)
summary(mod)
attributes(mod)
coef(mod)
confint(mod)
rmse<- sqrt(mean((train$SalePrice - train$YearBuilt)^2))
print(rmse)
ggplot(train,aes(YearBuilt,SalePrice))+
geom_point()+
geom_smooth(method='lm', se=FALSE)+
scale_y_continuous(labels = scales::comma)+
theme_minimal()+
labs(title="Sale Price VS YearBuilt ",x="YearBuilt",y="Sale Price ($)")
mod <- lm(SalePrice ~ YearBuilt ,data= train)
summary(mod)
attributes(mod)
coef(mod)
confint(mod)
rmse<- sqrt(mean((train$SalePrice - train$YearBuilt)^2))
print(rmse)
model_train <- train %>% select(c("SalePrice","GrLivArea","Foundation","BedroomAbvGr","LotArea","YearBuilt","Neighborhood", "HouseStyle"))
model_test <- test %>% select(c("SalePrice","GrLivArea","Foundation","BedroomAbvGr","LotArea","YearBuilt","Neighborhood", "HouseStyle"))
#split data into train and test
library(caret)
index <- createDataPartition(model_train$SalePrice, p = .70, list = FALSE)
length(model_train$SalePrice)
# build a model1 only between price and the variable with highest correlation with price (i.e. Area_income)
lmModel_First <- lm(SalePrice ~ LotArea ,model_train)
print(lmModel_First)
# Validating Regression Coefficients and Models
summary(lmModel_First)
# visualize the model
plot(lmModel_First)
# predicting
model_test$PreditedPrice_1 <- predict(lmModel_First, model_test)
head(model_test[ , c("SalePrice", "PreditedPrice_1")])
# compute the residual mean square error (RMSE) as a way of evaluation
actual <- model_test$SalePrice
preds <- model_test$PreditedPrice_1
Model_one_RMSE <- RMSE(preds, actual)
#Model_one_RMSE_formula <- sqrt(mean((preds - actual)^2))
# build a linear model between the price and all of the other stronly corr variables
lmModel_second <-  lm(
SalePrice~
GrLivArea+
Foundation+
BedroomAbvGr+
LotArea+
YearBuilt+
Neighborhood+
HouseStyle ,model_train)
summary(lmModel_second)
# Validating Regression Coefficients and Models
summary(lmModel_second)
# predicting
model_test$PreditedPrice_2 <- predict(lmModel_second, model_test)
head(model_test[ , c("SalePrice", "PreditedPrice_1",  "PreditedPrice_2")])
# RMSE
actual <- model_test$SalePrice
preds <- model_test$PreditedPrice_2
Mode2_one_RMSE<- RMSE(preds, actual)
#Mode2_one_RMSE_formula <- sqrt(mean((preds - actual)^2))
plot(lmModel_second)
# post assumptions
# Visualise the distribution of the residuals for both of the models
#residuals <- log10(residuals)
hist(lmModel_First$residuals, color = "grey")
hist(lmModel_second$residuals, color = "grey")
ggplot(lmModel_First, aes(lmModel_First$residuals)) +
geom_histogram(aes(y = ..density..), fill = "#C99800") +
geom_density(color = "blue")
ggplot(lmModel_second, aes(lmModel_second$residuals)) +
geom_histogram(aes(y = ..density..), fill = "#00BCD8") +
geom_density(color = "red")
